{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## <center> Q 1 – Particle Filter </center>\n",
    "##### In this assignment you will implement in Python a complete particle filter. A code skeleton with the work flow of the particle filter is provided for you. A visualization of the state of the particle filter is also provided by the framework. The following folders are included in the assignment framework.zip archive:\n",
    "##### data This folder contains files representing the world definition (i.e. landmarks) and sensor readings used by the filter.\n",
    "##### code This folder contains the particle filter framework with stubs for you to complete.\n",
    "##### You can run the particle filter in the terminal: python monte_carlo_localization.py. It will not work properly until you completed the blanks in the code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <center> 1.a [3 marks] </center>\n",
    "##### We used dictionaries to read in the sensor and landmark data. Dictionaries provide an easier way to access data structs based on single or multiple keys. The read_sensor_data and read_world_data functions in the read_files.py file read the data from the files and create a dictionary for each of them with time stamps as the primary keys.\n",
    "##### Familiarize yourself with the code of the two functions by commenting on the meaning of each of their lines. Please also answer the following question: How many landmarks does world_data.dat contain and how many time stamps does sensor_data.dat contain?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "sensor_data.dat: 331 Timestamps with an odometry-measurement (r1, t, r2 - the rotation and translation parameters) and a varying number of sensor-measurements(id, range, bearing - the landmarks id and the range and bearing from the sensor).\n",
    "world_data.dat: 9 Landmarks. One per row with the id in the first and x,y coordinates in the second and third column."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <center> 1.b [5 marks] </center>\n",
    "##### Fill in the code blank in the sample_odometry_motion_model function by implementing the odometry motion model as presented in the lecture ”‘Probabilistic Motion Models 2”’, slide 13. Comment on the meaning of each line. The function samples new particle positions based on the old positions, the odometry measurements δrot1, δtrans and δrot2 and the motion noise. This is done by passing each particle of the old set through the motion model in turn. Use the numpy.random.normal function for the motion noise and the parameters: [α1, α2, α3, α4] = [0.1, 0.1, 0.05, 0.05]. The function returns the new set of parameters after motion update. Please also answer the following question: What is the value of the pose of the first particle drawn from the motion model?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The odometry_motion_model was implemented according to the slides and Lab2 T2.\n",
    "\n",
    "The pose of the first particle drawn from the motion model is: [x, y, theta] = [ 3.657 2.966 -0.344 ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <center> 1.c [5 marks] </center>\n",
    "##### Fill in the compute_importance_weights function and comment on the meaning of each line. This function implements the measurement update step of a particle filter as presented in the lecture ”Probabilistic Sensor Models - 2”’, slide 25, using a range-only sensor. It takes as input positions lk and observations zk of landmarks. Note that individual observations are independent given a particle position xi (see ”‘Probabilistic Sensor Models 1”’, slide 39). The function returns a list of weights for the particle set. See slide 11 of the lecture ”‘Localization - Particle Filter - 2”’ for the definition of weights wi. Instead of calculating a probability, it is sufficient to calculate the likelihood p(z | x, l). Use the scipy.stats.norm.pdf function to evaluate this probability density function. The standard deviation of the normal distribution is σr = 0.2. Please also answer the following question: What is the value of the weight of the first particle drawn from the motion model?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The method for updating the importance weights was implemented as in the Lecture \"Probabilistic Sensor Models - 2\".\n",
    "\n",
    "The weight of the first particle has the weight: w_1 =  0.0000945"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <center> 1.d [5 marks] </center>\n",
    "##### Fill in the resampling function by implementing stochastic universal sampling as presented in the lecture ”‘Localization - Particle Filter - 3”’, slide 8. Comment on the meaning of each line. The function takes as an input a set of particles and the corresponding weights, and returns a sampled set of particles. Use the numpy.random.uniform function to draw random number r. Please also answer the following question: What is the value of the pose of the first resampled particle?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To ensure to sample particles that are likely to indicate the right position of the robot the stochastic universal sampling was implemented according to Lecture \"Localization Filter - 3\".\n",
    "\n",
    "The pose of the first resampled particle has the value: [x, y, theta] = [ 3.672 1.874 2.247 ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <center> 1.e [2 marks] </center>\n",
    "##### Use the plot_filter_state function to evaluate your completed particle filter implementation on the data provided in the folder data. What is the value of the robot pose estimate for the time stamps 0, 5, and 10? Save the resulting visualizations of the particle filter state for the time stamps 0, 5, and 10 as PDF-files to your report."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Output:\n",
    "Reading landmark positions\n",
    "Reading sensor data\n",
    "**The robot pose estimate for time stamp  0  is [x, y, theta] = [ 5.443 5.044 -2.797 ]**\n",
    "The robot pose estimate for time stamp  1  is [x, y, theta] = [ 1.313 0.690 -2.476 ]\n",
    "First particle drawn from the motion model has the pose [x, y, theta] = [ 3.657 2.966 -0.344 ]\n",
    "First particle drawn from the motion model has the weight w_1 =  0.0000945\n",
    "First resampled particle has the pose [x, y, theta] = [ 3.672 1.874 2.247 ]\n",
    "The robot pose estimate for time stamp  2  is [x, y, theta] = [ 0.428 0.294 -2.976 ]\n",
    "The robot pose estimate for time stamp  3  is [x, y, theta] = [ 0.361 0.196 -2.772 ]\n",
    "The robot pose estimate for time stamp  4  is [x, y, theta] = [ 0.184 0.185 -2.871 ]\n",
    "**The robot pose estimate for time stamp  5  is [x, y, theta] = [ 2.188 1.377 3.065 ]**\n",
    "The robot pose estimate for time stamp  6  is [x, y, theta] = [ 3.137 1.978 2.880 ]\n",
    "The robot pose estimate for time stamp  7  is [x, y, theta] = [ 3.041 2.140 2.730 ]\n",
    "The robot pose estimate for time stamp  8  is [x, y, theta] = [ 2.939 2.142 2.837 ]\n",
    "The robot pose estimate for time stamp  9  is [x, y, theta] = [ 0.469 0.209 0.514 ]\n",
    "**The robot pose estimate for time stamp  10  is [x, y, theta] = [ 0.558 0.271 0.614 ]**\n",
    "The robot pose estimate for time stamp  11  is [x, y, theta] = [ 0.643 0.339 0.710 ]\n",
    "The robot pose estimate for time stamp  12  is [x, y, theta] = [ 0.721 0.416 0.763 ]\n",
    "The robot pose estimate for time stamp  13  is [x, y, theta] = [ 0.790 0.496 0.864 ]\n",
    "The robot pose estimate for time stamp  14  is [x, y, theta] = [ 0.850 0.585 0.969 ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![alt text](./output/png/particle_filter_state_for_time_stamp_0.png)\n",
    "At timestep 0 it can be seen that the estimated robot positions are randomly distributied. This is according to the initialization.\n",
    "The arrow indicating the pose estimate is in the middle of the grid.\n",
    "The robot pose estimate for time stamp 0 is [x, y, theta] = [ 5.443 5.044 -2.797 ]\n",
    "![alt text](./output/png/particle_filter_state_for_time_stamp_5.png)\n",
    "After only 5 timesteps the particle filter has narrowed down the possible estimates to 2 bigger clusters and the arrow on a third that contains only one estimate.\n",
    "![alt text](./output/png/particle_filter_state_for_time_stamp_10.png)\n",
    "At timestep 10 the particle filter estimates all robot positions in one cluster in very close proximity."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementation Tips\n",
    "For accessing the sensor data from the sensor readings dictionary, you can use sensor_readings[timestamp,’sensor’][’id’] sensor_readings[timestamp,’sensor’][’range’] sensor_readings[timestamp,’sensor’][’bearing’] and for odometry you can access the dictionary as sensor_readings[timestamp,’odometry’][’r1’] sensor_readings[timestamp,’odometry’][’t’] sensor_readings[timestamp,’odometry’][’r2’] For accessing the landmark positions from the landmarks dictionary, you can use position_x = landmarks[id][0] position_y = landmarks[id][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attached Code\n",
    "(only missing)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### read_fildes.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.a\n",
    "\"\"\"\n",
    "def read_world_data(filename):\n",
    "    # Reads the world definition and returns a list of landmarks, our \"map\".\n",
    "    #\n",
    "    # The returned dict contains a list of landmarks with the\n",
    "    # following information: {id, [x, y]}\n",
    "\n",
    "    # initialize dict\n",
    "    landmarks = dict()\n",
    "\n",
    "    # open the file with the provided filename\n",
    "    f = open(filename)\n",
    "\n",
    "    # read file line by line\n",
    "    for line in f:\n",
    "        # split by new line\n",
    "        line_s = line.split('\\n')\n",
    "        # split again by space\n",
    "        line_spl = line_s[0].split(' ')\n",
    "        # add new key value pair to the dict\n",
    "        # key is the id of the landmark (first column)\n",
    "        # value is a list of the x and y coordinates of the landmark (second and thirdcolumn)\n",
    "        landmarks[int(line_spl[0])] = [float(line_spl[1]), float(line_spl[2])]\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def read_sensor_data(filename):\n",
    "    # Reads the odometry and sensor readings from a file.\n",
    "    #\n",
    "    # The data is returned in a dict where the u_t and z_t are stored\n",
    "    # together as follows:\n",
    "    #\n",
    "    # {odometry,sensor}\n",
    "    #\n",
    "    # where \"odometry\" contains the fields r1, r2, t which contain the values of\n",
    "    # the motion model variables of the same name, and sensor is a list of\n",
    "    # sensor readings with id, range, bearing as values.\n",
    "    #\n",
    "    # The odometry and sensor values are accessed as follows:\n",
    "    # odometry_data = sensor_readings[timestep, 'odometry']\n",
    "    # sensor_data = sensor_readings[timestep, 'sensor']\n",
    "\n",
    "    # initialize dict and lists\n",
    "    sensor_readings = dict()\n",
    "\n",
    "    lm_ids = []\n",
    "    ranges = []\n",
    "    bearings = []\n",
    "\n",
    "    # flag to skip saving sensordata to dict, because we dont have the sensordata at this step\n",
    "    first_time = True\n",
    "    # initialize timestamp to have the sensor readings in order\n",
    "    timestamp = 0\n",
    "    # open the file with the provided filename\n",
    "    f = open(filename)\n",
    "\n",
    "    # read file line by line\n",
    "    for line in f:\n",
    "\n",
    "        # split line by newline and space\n",
    "        line_s = line.split('\\n')\n",
    "        line_spl = line_s[0].split(' ')\n",
    "\n",
    "        # line starts with \"ODOMETRY\" handle odometry data\n",
    "        if line_spl[0] == 'ODOMETRY':\n",
    "            # write odometry data from the line to the dict for the current timestamp\n",
    "            sensor_readings[timestamp, 'odometry'] = {'r1': float(line_spl[1]),\n",
    "                                                      't': float(line_spl[2]),\n",
    "                                                      'r2': float(line_spl[3])}\n",
    "            # when reading the first odometry data we have no sensor data\n",
    "            if first_time:\n",
    "                first_time = False\n",
    "            # when its not the first time we have sensor data and write it to the dict and reset the lists for\n",
    "            else:\n",
    "                sensor_readings[timestamp, 'sensor'] = {'id': lm_ids,\n",
    "                                                        'range': ranges,\n",
    "                                                        'bearing': bearings}\n",
    "                lm_ids = []\n",
    "                ranges = []\n",
    "                bearings = []\n",
    "\n",
    "            # increment timestamp only for every odometry line\n",
    "            timestamp = timestamp + 1\n",
    "\n",
    "        # line starts with \"SENSOR\" handle sensor data\n",
    "        if line_spl[0] == 'SENSOR':\n",
    "            # append data to the lists\n",
    "            lm_ids.append(int(line_spl[1]))\n",
    "            ranges.append(float(line_spl[2]))\n",
    "            bearings.append(float(line_spl[3]))\n",
    "\n",
    "        # write the last information we have of the sensor data to the previous timestamp\n",
    "        sensor_readings[timestamp - 1, 'sensor'] = {'id': lm_ids,\n",
    "                                                    'range': ranges,\n",
    "                                                    'bearing': bearings}\n",
    "\n",
    "    return sensor_readings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### monte_carlo_localization.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from read_files import read_world_data, read_sensor_data\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\" Q 1 -- Particle Filter\n",
    "\"\"\"\n",
    "\n",
    "# DO NOT DELETE THIS LINE\n",
    "np.random.seed(123)\n",
    "\n",
    "# plot settings, interactive plotting mode\n",
    "plt.axis([-1, 12, 0, 10])\n",
    "plt.ion()\n",
    "plt.show()\n",
    "\n",
    "# global variable timestamp\n",
    "timestamp = 0\n",
    "\n",
    "\n",
    "def plot_filter_state(particles, landmarks, map_limits):\n",
    "    # Visualizes the particle filter state.\n",
    "    #\n",
    "    # Displays the particle cloud, the mean position and the landmarks.\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for particle in particles:\n",
    "        xs.append(particle['x'])\n",
    "        ys.append(particle['y'])\n",
    "\n",
    "    # landmark positions\n",
    "    lx = []\n",
    "    ly = []\n",
    "\n",
    "    for i in range(len(landmarks)):\n",
    "        # landmark indices start at one\n",
    "        lx.append(landmarks[i + 1][0])\n",
    "        ly.append(landmarks[i + 1][1])\n",
    "\n",
    "    # mean pose as current estimate\n",
    "    estimated_pose = mean_pose(particles)\n",
    "\n",
    "    # plot filter state\n",
    "    plt.clf()\n",
    "    plt.plot(xs, ys, 'r.')\n",
    "    plt.plot(lx, ly, 'bo', markersize=10)\n",
    "    plt.title(f\"Timestep: {timestamp}\")\n",
    "    plt.quiver(estimated_pose[0], estimated_pose[1], np.cos(estimated_pose[2]), np.sin(estimated_pose[2]), angles='xy',\n",
    "               scale_units='xy')\n",
    "    plt.axis(map_limits)\n",
    "\n",
    "    # for the first fifteen estimates\n",
    "    if timestamp < 15:\n",
    "        # output each on the console\n",
    "        print(\"The robot pose estimate for time stamp \", timestamp, \" is [x, y, theta] = [\",\n",
    "              \"{:.3f}\".format(estimated_pose[0]), \"{:.3f}\".format(estimated_pose[1]),\n",
    "              \"{:.3f}\".format(estimated_pose[2]), \"]\")\n",
    "        # save state of the particle filter\n",
    "        plt.savefig(\"./output/pdf/particle_filter_state_for_time_stamp_\" + str(timestamp) + \".pdf\")\n",
    "    plt.savefig(\"./output/png/particle_filter_state_for_time_stamp_\" + str(timestamp) + \".png\")\n",
    "\n",
    "    plt.pause(0.01)\n",
    "\n",
    "\n",
    "def initialize_particles(num_particles, map_limits):\n",
    "    # randomly initialize the particles inside the map limits\n",
    "\n",
    "    particles = []\n",
    "\n",
    "    for i in range(num_particles):\n",
    "        particle = dict()\n",
    "\n",
    "        # draw x,y and theta coordinate from uniform distribution\n",
    "        # inside map limits\n",
    "        particle['x'] = np.random.uniform(map_limits[0], map_limits[1])\n",
    "        particle['y'] = np.random.uniform(map_limits[2], map_limits[3])\n",
    "        particle['theta'] = np.random.uniform(-np.pi, np.pi)\n",
    "\n",
    "        particles.append(particle)\n",
    "\n",
    "    return particles\n",
    "\n",
    "\n",
    "def mean_pose(particles):\n",
    "    # calculate the average pose of a particle set.\n",
    "    #\n",
    "    # for x and y, the average position is the mean of the particle coordinates\n",
    "    #\n",
    "    # for theta, we cannot simply average the angles because of the wraparound\n",
    "    # (jump from -pi to pi). Therefore, we generate unit vectors from the\n",
    "    # angles and calculate the angle of their mean\n",
    "\n",
    "    # save x and y particle coordinates\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    # save unit vectors corresponding to orientations of particles\n",
    "    vxs_theta = []\n",
    "    vys_theta = []\n",
    "\n",
    "    for particle in particles:\n",
    "        xs.append(particle['x'])\n",
    "        ys.append(particle['y'])\n",
    "\n",
    "        # create unit vector from particle orientation\n",
    "        vxs_theta.append(np.cos(particle['theta']))\n",
    "        vys_theta.append(np.sin(particle['theta']))\n",
    "\n",
    "    # compute average coordinates\n",
    "    mean_x = np.mean(xs)\n",
    "    mean_y = np.mean(ys)\n",
    "    mean_theta = np.arctan2(np.mean(vys_theta), np.mean(vxs_theta))\n",
    "\n",
    "    return [mean_x, mean_y, mean_theta]\n",
    "\n",
    "\"\"\"\n",
    "1.b\n",
    "\"\"\"\n",
    "def sample_odometry_motion_model(odometry, particles):\n",
    "    # function according to the slides with particle as x and the odometry measurements as u\n",
    "    def sample_motion_model(particle):\n",
    "        # compute new odometry data\n",
    "        delta_hat_rot1 = delta_rot1 + np.random.normal(0, noise[0] * abs(delta_rot1) + noise[1] * delta_trans)\n",
    "        delta_hat_trans = delta_trans + np.random.normal(0, noise[2] * delta_trans + noise[3] * (\n",
    "                    abs(delta_rot1) + abs(delta_rot2)))\n",
    "        delta_hat_rot2 = delta_rot2 + np.random.normal(0, noise[0] * abs(delta_rot2) + noise[1] * delta_trans)\n",
    "\n",
    "        # update particle pose parameters based on updated odometry above\n",
    "        x_new = particle['x'] + delta_hat_trans * np.cos(particle['theta'] + delta_hat_rot1)\n",
    "        y_new = particle['y'] + delta_hat_trans * np.sin(particle['theta'] + delta_hat_rot1)\n",
    "        theta_new = particle['theta'] + delta_hat_rot1 + delta_hat_rot2\n",
    "\n",
    "        # return dict of updated particle\n",
    "        return {'x': x_new, 'y': y_new, 'theta': theta_new}\n",
    "\n",
    "\n",
    "    # measurements\n",
    "    delta_rot1 = odometry['r1']\n",
    "    delta_trans = odometry['t']\n",
    "    delta_rot2 = odometry['r2']\n",
    "\n",
    "    # motion noise parameters: [alpha1, alpha2, alpha3, alpha4]\n",
    "    noise = [0.1, 0.1, 0.05, 0.05]\n",
    "\n",
    "    # generate new particle set after motion update\n",
    "    new_particles = []\n",
    "\n",
    "    # iterate over particles\n",
    "    for particle in particles:\n",
    "        # sample new particle pose from sample motion model algorithm\n",
    "        # from slide 13 of lecture \"Probabilistic Motion Models - 2\"\n",
    "        # append new particle pose to new particles list\n",
    "        new_particles.append(sample_motion_model(particle))\n",
    "\n",
    "    # Output the pose of the first particle drawn from the motion model\n",
    "    if timestamp == 1:\n",
    "        print(\"First particle drawn from the motion model has the pose [x, y, theta] = \"\n",
    "              \"[\",\n",
    "              \"{:.3f}\".format(new_particles[0]['x']),\n",
    "              \"{:.3f}\".format(new_particles[1]['y']),\n",
    "              \"{:.3f}\".format(new_particles[2]['theta']),\n",
    "              \"]\")\n",
    "\n",
    "    return new_particles\n",
    "\n",
    "\"\"\"\n",
    "1.c\n",
    "\"\"\"\n",
    "def compute_importance_weights(sensor_data, particles, landmarks):\n",
    "    # Calculates the observation likelihood of all particles, given the\n",
    "    # particle and landmark positions and sensor readings\n",
    "    #\n",
    "    # The sensor model used is range only.\n",
    "\n",
    "    sigma_r = 0.2\n",
    "\n",
    "    # measured landmark ids and ranges\n",
    "    ids = sensor_data['id']\n",
    "    ranges = sensor_data['range']\n",
    "\n",
    "    weights = []\n",
    "\n",
    "    for particle in particles:\n",
    "\n",
    "        # Initialize particles importance weight as 1\n",
    "        importance_weight = 1\n",
    "\n",
    "        for id, range in zip(ids, ranges):\n",
    "            # Calculate expected distance by euclidean distance between landmark and particle\n",
    "            expected_distance = np.linalg.norm(np.array(landmarks[id])-np.array([particle[\"x\"], particle[\"y\"]])) # euchlidean distance - l2 norm\n",
    "\n",
    "            # Calculate the likelihood according to 1.b use a gaussian distribution\n",
    "            likelihood = scipy.stats.norm.pdf(range, expected_distance, sigma_r)\n",
    "\n",
    "            # update the particles importance weight by the likelihood of the current landmark\n",
    "            importance_weight *= likelihood\n",
    "\n",
    "        weights.append(importance_weight)\n",
    "\n",
    "    # normalize weights\n",
    "    normalizer = sum(weights)\n",
    "    weights = weights / normalizer\n",
    "\n",
    "    # output the weight of the first particle drawn from the motion model\n",
    "    if timestamp == 1:\n",
    "        print(\"First particle drawn from the motion model has the weight w_1 = \", \"{:.7f}\".format(weights[0]))\n",
    "\n",
    "    return weights\n",
    "\n",
    "\"\"\"\n",
    "1.d\n",
    "\"\"\"\n",
    "def resampling(particles, weights):\n",
    "    # Returns a new set of particles obtained by stochastic\n",
    "    # universal sampling according to particle weights.\n",
    "\n",
    "    \"\"\"\n",
    "    Line 2\n",
    "    Initialize new particles and the cummulative sum of the weights\n",
    "    \"\"\"\n",
    "    new_particles = []\n",
    "    cumsum = [weights[0]]\n",
    "    n = len(particles)\n",
    "\n",
    "    \"\"\"\n",
    "    Line 3, 4\n",
    "    compute cummulative sums\n",
    "    numpy function would have been possible (even faster)\n",
    "    but I try to stay consistent with the algorithm on slide 8\n",
    "    \"\"\"\n",
    "    for i in range(1, n):\n",
    "        cumsum.append(cumsum[i - 1] + weights[i])\n",
    "\n",
    "    \"\"\"\n",
    "    Line 5\n",
    "    Initialize first arrow u with random number r\n",
    "    reset index to 0, slides start to count at 1, here 0\n",
    "    \"\"\"\n",
    "    # draw random number from ]0,1/N]\n",
    "    r = np.random.uniform(0, 1 / len(particles))\n",
    "    # initialize arrow\n",
    "    u = [r]\n",
    "    # reset index\n",
    "    i = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Line 6\n",
    "    Loop over all particles\n",
    "    \"\"\"\n",
    "    for j in range(n):\n",
    "\n",
    "        \"\"\"\n",
    "        Line 7\n",
    "        Skip invertals until particle reached (until arrow < cumsum weights)\n",
    "        \"\"\"\n",
    "        while u[j] > cumsum[i]:\n",
    "            \"\"\"\n",
    "            Line 8\n",
    "            increment to skip\n",
    "            \"\"\"\n",
    "            i += 1\n",
    "        \"\"\"\n",
    "        Line 9\n",
    "        Save particle as sampled\n",
    "        \"\"\"\n",
    "        new_particles.append(particles[i])\n",
    "\n",
    "        \"\"\"\n",
    "        Line 10\n",
    "        Increment arrow\n",
    "        \"\"\"\n",
    "        next_u = u[j] + 1/n\n",
    "        u.append(next_u)\n",
    "\n",
    "    # Output the pose of the first resampled particle\n",
    "    if timestamp == 1:\n",
    "        print(\"First resampled particle has the pose [x, y, theta] = [\", \"{:.3f}\".format(new_particles[0]['x']),\n",
    "              \"{:.3f}\".format(new_particles[1]['y']), \"{:.3f}\".format(new_particles[2]['theta']), \"]\")\n",
    "\n",
    "    return new_particles\n",
    "\n",
    "\n",
    "def main():\n",
    "    # implementation of a particle filter for monte carlo localization\n",
    "\n",
    "    global timestamp\n",
    "\n",
    "    print(\"Reading landmark positions\")\n",
    "    landmarks = read_world_data(\"./data/world_data.dat\")\n",
    "\n",
    "    print(\"Reading sensor data\")\n",
    "    sensor_readings = read_sensor_data(\"./data/sensor_data.dat\")\n",
    "\n",
    "    # initialize the particles\n",
    "    map_limits = [-1, 12, 0, 10]\n",
    "    particles = initialize_particles(1000, map_limits)\n",
    "\n",
    "    # run particle filter for all timestamps\n",
    "    for timestamp in range(len(sensor_readings) // 2):\n",
    "        # plot the current state\n",
    "        plot_filter_state(particles, landmarks, map_limits)\n",
    "\n",
    "        # predict particles by sampling from motion model with odometry measurements\n",
    "        new_particles = sample_odometry_motion_model(sensor_readings[timestamp, 'odometry'], particles)\n",
    "\n",
    "        # compute importance weights according to sensor model\n",
    "        weights = compute_importance_weights(sensor_readings[timestamp, 'sensor'], new_particles, landmarks)\n",
    "\n",
    "        # resample new particle set according to importance weights\n",
    "        particles = resampling(new_particles, weights)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}