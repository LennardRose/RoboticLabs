{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <center> Artificial Intelligence in Robotics - Laboratory 4 </center>\n",
    "### <center> By Lennard Rose 5122737 </center>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <center> T 1 – Simultaneous Localization and Mapping </center>\n",
    "##### The FastSLAM approach is a Rao-Blackwellized particle filter for SLAM (Simultaneous Localization And Mapping). The pose of the robot in the environment is represented by a particle filter. In addition, each particle carries a map of the environment, which it uses for localization. In the case of landmark-based FastSLAM, the map is represented by Extended Kalman filters (EKFs) that estimate the mean positions and covariances of landmarks.\n",
    "##### In this computer practical, you are to implement one step of the landmark-based FastSLAM algorithm as detailed in the supporting PDF file ”‘fastslam algorithm.pdf”’. Assume the feature correspondences are known. A code skeleton in Python with the work flow of the algorithm is provided for you. A visualization of the state of FastSLAM is also provided by the framework.\n",
    "##### The following folders are included in the lab 4 framework.zip archive:\n",
    "##### data This folder contains files representing the world definition (i.e. landmarks) and sensor readings used by FastSLAM.\n",
    "##### code This folder contains the FastSLAM framework with a stub for you to complete\n",
    "##### You can run the FastSLAM framework in the terminal: python feature_based_fast_ slam.py. It will not work properly until you completed the blanks in the code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <center> 1.a [1.0 marks] </center>\n",
    "##### Familiarize yourself with the existing code skeleton. What is the value of the robot pose that the particle filter is initialized with? Why does it make sense to choose this value? Is sampling from the motion model in sample_odometry_motion_model or the resampling procedure in resampling significantly different from those in Monte Carlo localization (see Take Home Exam / Assignment)?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The particle filter is initialized with the pose values $ \\begin{pmatrix}x \\\\ y \\\\ \\theta\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix} $ because after initialization, it has no further information about the environment. With further computations the robot will build its environment model around this referencepoint $ \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix} $. This value makes sense because in mathmatics 0 is used as the origin and normalizations often aim to set for example the mean to this point for further references.\n",
    "\n",
    "The resampling procedure differs in the implementation from the take home exam. The general procedure seems to be similar.\n",
    "The sampling from the motion model is different, as it also stores trajectory information. For this, the x and y position of the previous particle is saved so the information of how it moved in the environment is available.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <center> 1.b [6.0 marks] </center>\n",
    "##### Fill in the sensor_update function and comment on the meaning of each line. sensor_ update implements the measurement update of the Rao-Blackwellized particle filter using range and bearing measurements. It takes the particles and landmark observations and updates the map of each particle and calculates its weight w. The noise of the sensor measurements is given by a diagonal matrix $$Q_t = \\left(\\begin{array}{cc} 0.1 & 0 \\\\ 0 & 0.1 \\end{array}\\right)$$\n",
    "##### When updating the map of a particle, each measurement can belong either to a known landmark or to a new one. If a measurement belongs to a new landmark, a 2 × 2 EKF must be initialized according to lines 7 to 9 on slide 1 of the supporting PDF file. When initializing the mean position, you should take inspiration from ”‘Tutorial 4 - Q 2”’. Use functions from numpy.linalg to initialize the landmark covariance. A function measurement_prediction_and_jacobian is provided to you in the framework so that you don’t need to implement a measurement model. If, on the other hand, a measurement belongs to a known landmark, the mean position and covariance of the landmark must be updated according to the EKF Correction step (see lines 12 to 17 on slide 3 of the supporting PDF file). Use the provided function measurement_prediction_and_jacobian. To correctly compute the difference between measured and expected bearing, you should use the angle_diff function also available in the framework. After the Correction step, the likelihood of the measurement must be computed according to line 18 on slide 3 of the supporting PDF file. Use the function scipy.stats.multivariate_normal.pdf to this end. Use this likelihood to update the importance weight of the particle at last. Please also answer this question: What is the value of the best particle’s estimate of the robot pose and how many landmarks have been observered by the end of time stamp 0?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function sensor update was completed using the provided materials (Tutorial 4- Q2 and Lab 4 FastSLAM Algorithm.pdf). The Line numbers in the comments are refering to the line numbers in the long version (Page 3) of the algorithm.\n",
    "Because of the folder structure, and because this notebook would plot every step, feature_based_fast_slam.py is not called here.\n",
    "The funktion sensor_update updates the particle landmark positions and weigths.\n",
    "\n",
    "Unobserved landmarks are initialized with the mean as the calculated landmark positions and the covariance with the corresponding formula based on the jacobian and the noise (Lines 6 - 9).\n",
    "\n",
    "For observed landmarks, the mean and the covariance, as well as the weight are updated corresponding to Line 12-18.\n",
    "First, the measurements are calculated with respect to an uncertainty factor (Line 12-15 including the Kalman gain)\n",
    "Then, the mean and the covariance of the landmark are updated based on the Kalman gain and the measurements/predictions (Line 16, 17). The mean is then used for the likelihood to update the particles weight (Line 18), the covariance is updated for the next updating steps.\n",
    "\n",
    "The output after update step 0 is:\n",
    "Robot pose estimate of best particle for time stamp  0  is [x, y, theta] = [ 0.110 0.009 0.081 ]\n",
    "Estimated mean position of landmark  1  for time stamp  0  is [x, y] = [ 2.034 0.805 ]\n",
    "Estimated mean position of landmark  2  for time stamp  0  is [x, y] = [ 0.305 4.016 ]\n",
    "So the best particles estimate of the robot pose is: [x, y, theta] = [ 0.110 0.009 0.081 ]. Two landmarks have been observed. All unobserved landmarks are initialized with the values explained above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <center> 1.c [1.0 marks] </center>\n",
    "##### Use the plot_fast_slam_state function to evaluate your completed FastSLAM implementation on the data provided in the folder data. Comment on how the uncertainties in the best particle’s map, i.e. its Kalman filters, evolve between time stamps 0 and 7. Put this in relation to the main effects of the EKF Correction step. Save the state plots of FastSLAM at time stamps 0 and 7 as PDF-files to your lab report."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![alt text](./output/png/fast_slam_state_for_time_stamp_0.png)\n",
    "</br>\n",
    "After initialization at timestep 0 the kalman filter only had one measurement to estimate the landmarks exact positions. Therefore the uncertainties are relatively high. In the next steps, with the particles moving according to the motion model, more information of the landmark is provided. Based on this informations and the difference to the expected positions the filter is updated and the landmark estimations get more precise as seen in the next figure for the time step 7:\n",
    "</br>\n",
    "![alt text](./output/png/fast_slam_state_for_time_stamp_7.png)\n",
    "</br>\n",
    "The error elipse that indicates the estimates possible positions are less spread around the actual position.\n",
    "With more iterations, the uncertainty will further decrease."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <center> 1.d [1.0 marks] </center>\n",
    "##### How does the best particle’s map of the environment change between time stamps 7 and 8? What is the reason for the differences in the dimensions of the error ellipses in the Kalman filters at time stamp 8? What uncertainties account for the largest error ellipse at this time stamp? Save the state plot at time stamp 8 as a PDF-file to your lab report."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![alt text](./output/png/fast_slam_state_for_time_stamp_7.png)\n",
    "</br>\n",
    "After the uncertainties decreased leading to the small elipses we see in timestep 7 a new landmark is observed.\n",
    "</br>\n",
    "![alt text](./output/png/fast_slam_state_for_time_stamp_8.png)\n",
    "</br>\n",
    "The new Landmark in timestep 8 got initialized with a high uncertainty, just because there were to little measurements to estimate an accurate position. Leading to the large error elipse. The two other measurements were updated multiple times, leading to lower uncertainty, therefore smaller elipses."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <center> 1.e [1.0 marks] </center>\n",
    "##### Comment on how the particle filter’s uncertainty in the robot pose evolves from time stamp 0, to time stamp 100, to time stamp 250. In comparison, how does the uncertainty evolve between time stamps 250 and 300, and what is the name of the event that this is due to? Save the state plots for the time stamps 100, 250, and 300 as PDF-files to your lab report."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![alt text](./output/png/fast_slam_state_for_time_stamp_0.png)\n",
    "</br>\n",
    "After initialization the uncertainty is insignificant because the robot only sampled one time from the motion model.\n",
    "</br>\n",
    "\n",
    "![alt text](./output/png/fast_slam_state_for_time_stamp_100.png)\n",
    "</br>\n",
    "After 100 timesteps the uncertainty reached a non-negligible level, as seen on the figure, there are now multiple dots with possible positions.\n",
    "</br>\n",
    "![alt text](./output/png/fast_slam_state_for_time_stamp_250.png)\n",
    "</br>\n",
    "At timestep 250 there are two observable clusters of possible positions. The two clusters are not spreaded so far as in timestep 100, indicating lower uncertainty.\n",
    "</br>\n",
    "![alt text](./output/png/fast_slam_state_for_time_stamp_300.png)\n",
    "</br>\n",
    "\n",
    "The two clusters in timestep 300 are nearer to each other than in the previous figures, meaning the uncertainty has further decreased. This might be because previously observed landmarks are observed again, leading to greater certainty about the own position.\n",
    "\n",
    "Nevertheless, the landmarks are quite off in the above pictures. This is because they are always relative to the robot, the reference point."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![alt text](./output/png/fast_slam_state_for_time_stamp_0.png)\n",
    "</br>\n",
    "After initialization the uncertainty is insignificant because the robot only sampled one time from the motion model.\n",
    "</br>\n",
    "\n",
    "![alt text](./output/png/fast_slam_state_for_time_stamp_100.png)\n",
    "</br>\n",
    "After 100 timesteps the uncertainty reached a non-negligible level, as seen on the figure, there are now multiple dots with possible positions.\n",
    "</br>\n",
    "![alt text](./output/png/fast_slam_state_for_time_stamp_250.png)\n",
    "</br>\n",
    "At timestep 250 there are two observable clusters of possible positions. The two clusters are not spreaded so far as in timestep 100, indicating lower uncertainty.\n",
    "</br>\n",
    "![alt text](./output/png/fast_slam_state_for_time_stamp_300.png)\n",
    "</br>\n",
    "\n",
    "The two clusters in timestep 300 are nearer to each other than in the previous figures, meaning the uncertainty has further decreased. This might be because previously observed landmarks are observed again, leading to greater certainty about the own position.\n",
    "\n",
    "Nevertheless, the landmarks are quite off in the above pictures. This is because they are always relative to the robot, the reference point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Implementation Tips\n",
    "We used dictionaries to read in the sensor and landmark data. Dictionaries provide an easier way to access data structs based on single or multiple keys. The read_sensor_data and read_world_data functions in the read_files.py file read the data from the files and create a dictionary for each of them with time stamps as the primary keys.\n",
    "For accessing the sensor data from the sensor readings dictionary, you can use sensor_readings[timestamp,’sensor’][’id’] sensor_readings[timestamp,’sensor’][’range’] sensor_readings[timestamp,’sensor’][’bearing’]\n",
    "and for odometry you can access the dictionary as sensor_readings[timestamp,’odometry’][’r1’] sensor_readings[timestamp,’odometry’][’t’] sensor_readings[timestamp,’odometry’][’r2’]\n",
    "For accessing the landmark positions from the landmarks dictionary, you can use position_x = landmarks[id][0] position_y = landmarks[id][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Attached Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### read_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_world_data(filename):\n",
    "    # Reads the world definition and returns a list of landmarks, our 'map'.\n",
    "    #\n",
    "    # The returned dict contains a list of landmarks each with the\n",
    "    # following information: {id, [x, y]}\n",
    "\n",
    "    landmarks = dict()\n",
    "\n",
    "    f = open(filename)\n",
    "\n",
    "    for line in f:\n",
    "\n",
    "        line_s  = line.split('\\n')\n",
    "        line_spl  = line_s[0].split(' ')\n",
    "        landmarks[int(line_spl[0])] = [float(line_spl[1]),float(line_spl[2])]\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "def read_sensor_data(filename):\n",
    "    # Reads the odometry readings and sensor measurements from a file.\n",
    "    #\n",
    "    # The data is returned in a dict where the u_t and z_t are stored\n",
    "    # together as follows:\n",
    "    #\n",
    "    # {odometry,sensor}\n",
    "    #\n",
    "    # where \"odometry\" has the fields r1, r2, t which contain the values of\n",
    "    # the identically named motion model variables, and sensor is a list of\n",
    "    # sensor readings with id, range, bearing as values.\n",
    "    #\n",
    "    # The odometry and sensor values are accessed as follows:\n",
    "    # odometry_data = sensor_readings[timestep, 'odometry']\n",
    "    # sensor_data = sensor_readings[timestep, 'sensor']\n",
    "\n",
    "    sensor_readings = dict()\n",
    "\n",
    "    lm_ids =[]\n",
    "    ranges=[]\n",
    "    bearings=[]\n",
    "\n",
    "    first_time = True\n",
    "    timestamp = 0\n",
    "    f = open(filename)\n",
    "\n",
    "    for line in f:\n",
    "\n",
    "        line_s = line.split('\\n')\n",
    "        line_spl = line_s[0].split(' ')\n",
    "\n",
    "        if (line_spl[0]=='ODOMETRY'):\n",
    "\n",
    "            sensor_readings[timestamp,'odometry'] = {'r1':float(line_spl[1]),'t':float(line_spl[2]),'r2':float(line_spl[3])}\n",
    "\n",
    "            if first_time:\n",
    "                first_time = False\n",
    "\n",
    "            else:\n",
    "                sensor_readings[timestamp,'sensor'] = {'id':lm_ids,'range':ranges,'bearing':bearings}\n",
    "                lm_ids=[]\n",
    "                ranges = []\n",
    "                bearings = []\n",
    "\n",
    "            timestamp = timestamp+1\n",
    "\n",
    "        if(line_spl[0]=='SENSOR'):\n",
    "\n",
    "            lm_ids.append(int(line_spl[1]))\n",
    "            ranges.append(float(line_spl[2]))\n",
    "            bearings.append(float(line_spl[3]))\n",
    "\n",
    "        sensor_readings[timestamp-1,'sensor'] = {'id':lm_ids,'range':ranges,'bearing':bearings}\n",
    "\n",
    "    return sensor_readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### plot_and_save.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def compute_error_ellipse(position, sigma):\n",
    "\n",
    "    covariance = sigma[0:2,0:2]\n",
    "    eigenvals, eigenvecs = np.linalg.eig(covariance)\n",
    "\n",
    "    #get the largest eigenvalue and corresponding eigenvector\n",
    "    max_ind = np.argmax(eigenvals)\n",
    "    max_eigvec = eigenvecs[:,max_ind]\n",
    "    max_eigval = eigenvals[max_ind]\n",
    "\n",
    "    #get the smallest eigenvalue and corresponding eigenvector\n",
    "    min_ind = 0\n",
    "    if max_ind == 0:\n",
    "        min_ind = 1\n",
    "\n",
    "    min_eigvec = eigenvecs[:,min_ind]\n",
    "    min_eigval = eigenvals[min_ind]\n",
    "\n",
    "    #chi-square value for sigma confidence interval\n",
    "    chisquare_scale = 2.2789\n",
    "\n",
    "    #compute width and height of confidence ellipse\n",
    "    width = 2 * np.sqrt(chisquare_scale*max_eigval)\n",
    "    height = 2 * np.sqrt(chisquare_scale*min_eigval)\n",
    "    angle = np.arctan2(max_eigvec[1],max_eigvec[0])\n",
    "\n",
    "    #generate covariance ellipse\n",
    "    error_ellipse = Ellipse(xy=[position[0],position[1]], width=width, height=height, angle=angle/np.pi*180)\n",
    "    error_ellipse.set_alpha(0.25)\n",
    "\n",
    "    return error_ellipse\n",
    "\n",
    "def plot_slam_state(particles, landmarks, timestamp):\n",
    "    # Visualizes the state of the FastSLAM algorithm.\n",
    "    #\n",
    "    # Displays the particle cloud, the mean position and\n",
    "    # the estimated mean landmark positions and covariances.\n",
    "\n",
    "    draw_mean_landmark_poses = False\n",
    "\n",
    "    map_limits = [-1, 12, 0, 10]\n",
    "\n",
    "    #particle positions\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    #landmark mean positions\n",
    "    lxs = []\n",
    "    lys = []\n",
    "\n",
    "    for particle in particles:\n",
    "        xs.append(particle['x'])\n",
    "        ys.append(particle['y'])\n",
    "\n",
    "        for i in range(len(landmarks)):\n",
    "            landmark = particle['landmarks'][i+1]\n",
    "            lxs.append(landmark['mu'][0])\n",
    "            lys.append(landmark['mu'][1])\n",
    "\n",
    "    # ground truth landmark positions\n",
    "    lx=[]\n",
    "    ly=[]\n",
    "\n",
    "    for i in range (len(landmarks)):\n",
    "        lx.append(landmarks[i+1][0])\n",
    "        ly.append(landmarks[i+1][1])\n",
    "\n",
    "    # best particle\n",
    "    estimated = best_particle(particles)\n",
    "    robot_x = estimated['x']\n",
    "    robot_y = estimated['y']\n",
    "    robot_theta = estimated['theta']\n",
    "\n",
    "    # estimated traveled path of best particle\n",
    "    hist = estimated['history']\n",
    "    hx = []\n",
    "    hy = []\n",
    "\n",
    "    for pos in hist:\n",
    "        hx.append(pos[0])\n",
    "        hy.append(pos[1])\n",
    "\n",
    "    # plot FastSLAM state\n",
    "    plt.clf()\n",
    "\n",
    "    #particles\n",
    "    plt.plot(xs, ys, 'r.')\n",
    "\n",
    "    if draw_mean_landmark_poses:\n",
    "        # estimated mean landmark positions of each particle\n",
    "        plt.plot(lxs, lys, 'b.')\n",
    "\n",
    "    # estimated traveled path of best particle\n",
    "    plt.plot(hx, hy, 'r-')\n",
    "\n",
    "    # true landmark positions\n",
    "    plt.plot(lx, ly, 'b+',markersize=10)\n",
    "\n",
    "    # draw error ellipse of estimated landmark positions of best particle\n",
    "    for i in range(len(landmarks)):\n",
    "        landmark = estimated['landmarks'][i+1]\n",
    "\n",
    "        ellipse = compute_error_ellipse(landmark['mu'], landmark['sigma'])\n",
    "        plt.gca().add_artist(ellipse)\n",
    "\n",
    "    # draw pose of best particle\n",
    "    plt.quiver(robot_x, robot_y, np.cos(robot_theta), np.sin(robot_theta), angles='xy',scale_units='xy')\n",
    "\n",
    "    plt.axis(map_limits)\n",
    "\n",
    "    #for selected time steps\n",
    "    if timestamp == 0 or timestamp == 7 or timestamp == 8 or timestamp == 100 or timestamp == 250 or timestamp == 300:\n",
    "        #output the robot pose and map estimate of the best particle\n",
    "        print(\"Robot pose estimate of best particle for time stamp \", timestamp, \" is [x, y, theta] = [\", \"{:.3f}\".format(robot_x), \"{:.3f}\".format(robot_y), \"{:.3f}\".format(robot_theta), \"]\")\n",
    "        #output all estimated mean landmark positions\n",
    "        for i in range(len(landmarks)):\n",
    "            landmark = estimated['landmarks'][i+1]\n",
    "            if landmark['observed']:\n",
    "                print(\"Estimated mean position of landmark \", i+1, \" for time stamp \", timestamp,\" is [x, y] = [\", \"{:.3f}\".format(landmark['mu'][0]), \"{:.3f}\".format(landmark['mu'][1]), \"]\")\n",
    "        #save state of the particle filter\n",
    "        plt.savefig(\"../output/png/fast_slam_state_for_time_stamp_\" + str(timestamp) + \".png\")\n",
    "        plt.savefig(\"../output/pdf/fast_slam_state_for_time_stamp_\" + str(timestamp) + \".pdf\")\n",
    "\n",
    "    plt.pause(0.01)\n",
    "\n",
    "def best_particle(particles):\n",
    "    #find the particle with the highest weight\n",
    "\n",
    "    highest_weight = 0\n",
    "\n",
    "    best_particle = None\n",
    "\n",
    "    for particle in particles:\n",
    "        if particle['weight'] > highest_weight:\n",
    "            best_particle = particle\n",
    "            highest_weight = particle['weight']\n",
    "\n",
    "    return best_particle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### feature_based_fast_slam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from Lennard_Rose_AIIR_SS23.Lab_4.read_files import read_world_data, read_sensor_data\n",
    "from Lennard_Rose_AIIR_SS23.Lab_4.plot_and_save import *\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "\"\"\"\n",
    "T 1 -- Feature-based FastSLAM\n",
    "\"\"\"\n",
    "\n",
    "# DO NOT DELETE THIS LINE\n",
    "np.random.seed(123)\n",
    "\n",
    "# plot settings, interactive plotting mode\n",
    "plt.axis([-1, 12, 0, 10])\n",
    "plt.ion()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def initialize_particles(num_particles, num_landmarks):\n",
    "    # initialize particle filter with a robot pose and an empty map\n",
    "\n",
    "    particles = []\n",
    "\n",
    "    # create one particle at a time\n",
    "    for i in range(num_particles):\n",
    "        particle = dict()\n",
    "\n",
    "        # initialize pose\n",
    "        particle['x'] = 0\n",
    "        particle['y'] = 0\n",
    "        particle['theta'] = 0\n",
    "\n",
    "        # initial weight\n",
    "        particle['weight'] = 1.0 / num_particles\n",
    "\n",
    "        # particle history: modeling the robot's path\n",
    "        particle['history'] = []\n",
    "\n",
    "        # initialize landmarks of the particle\n",
    "        landmarks = dict()\n",
    "\n",
    "        # create one landmark at a time\n",
    "        for i in range(num_landmarks):\n",
    "            landmark = dict()\n",
    "\n",
    "            # initialize the landmark mean and covariance\n",
    "            landmark['mu'] = [0, 0]\n",
    "            landmark['sigma'] = np.zeros([2, 2])\n",
    "            landmark['observed'] = False\n",
    "\n",
    "            # landmark indices start at 1\n",
    "            landmarks[i + 1] = landmark\n",
    "\n",
    "        # add landmarks to particle\n",
    "        particle['landmarks'] = landmarks\n",
    "\n",
    "        # add particle to set\n",
    "        particles.append(particle)\n",
    "\n",
    "    return particles\n",
    "\n",
    "\n",
    "def sample_odometry_motion_model(odometry, particles):\n",
    "    # Updates the positions of the particles, based on the old positions, odometry\n",
    "    # measurements and motion noise (see \"Take Home Exam / Assignment\")\n",
    "\n",
    "    delta_rot1 = odometry['r1']\n",
    "    delta_trans = odometry['t']\n",
    "    delta_rot2 = odometry['r2']\n",
    "\n",
    "    # motion noise parameters: [alpha1, alpha2, alpha3, alpha4]\n",
    "    noise = [0.1, 0.1, 0.05, 0.05]\n",
    "\n",
    "    # compute standard deviations of motion noise\n",
    "    sigma_delta_rot1 = noise[0] * abs(delta_rot1) + noise[1] * delta_trans\n",
    "    sigma_delta_trans = noise[2] * delta_trans + noise[3] * (abs(delta_rot1) + abs(delta_rot2))\n",
    "    sigma_delta_rot2 = noise[0] * abs(delta_rot2) + noise[1] * delta_trans\n",
    "\n",
    "    # \"move\" each particle according to the odometry measurements plus sampled noise\n",
    "    for particle in particles:\n",
    "        # sample noisy motions\n",
    "        noisy_delta_rot1 = delta_rot1 + np.random.normal(0, sigma_delta_rot1)\n",
    "        noisy_delta_trans = delta_trans + np.random.normal(0, sigma_delta_trans)\n",
    "        noisy_delta_rot2 = delta_rot2 + np.random.normal(0, sigma_delta_rot2)\n",
    "\n",
    "        # remember last position as part of the path of the particle\n",
    "        particle['history'].append([particle['x'], particle['y']])\n",
    "\n",
    "        # compute new particle pose\n",
    "        particle['x'] = particle['x'] + noisy_delta_trans * np.cos(particle['theta'] + noisy_delta_rot1)\n",
    "        particle['y'] = particle['y'] + noisy_delta_trans * np.sin(particle['theta'] + noisy_delta_rot1)\n",
    "        particle['theta'] = particle['theta'] + noisy_delta_rot1 + noisy_delta_rot2\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def measurement_prediction_and_jacobian(particle, landmark):\n",
    "    # Calculate the expected measurement for a landmark\n",
    "    # and the Jacobian with respect to the landmark.\n",
    "\n",
    "    px = particle['x']\n",
    "    py = particle['y']\n",
    "    ptheta = particle['theta']\n",
    "\n",
    "    lx = landmark['mu'][0]\n",
    "    ly = landmark['mu'][1]\n",
    "\n",
    "    # compute expected range and bearing measurements (see \"Probabilistic Sensor Models - 2\", slide 25)\n",
    "    measured_range_exp = np.sqrt((lx - px) ** 2 + (ly - py) ** 2)\n",
    "    measured_bearing_exp = math.atan2(ly - py, lx - px) - ptheta\n",
    "\n",
    "    # create vector of expected measurements for Kalman correction\n",
    "    h = np.array([measured_range_exp, measured_bearing_exp])\n",
    "\n",
    "    # Calculate the Jacobian H of the measurement function h\n",
    "    # wrt the landmark location (see Tutorial 4 - Q 3)\n",
    "    H = np.zeros((2, 2))\n",
    "    H[0, 0] = (lx - px) / h[0]\n",
    "    H[0, 1] = (ly - py) / h[0]\n",
    "    H[1, 0] = (py - ly) / (h[0] ** 2)\n",
    "    H[1, 1] = (lx - px) / (h[0] ** 2)\n",
    "\n",
    "    return h, H\n",
    "\n",
    "\n",
    "def calculate_landmark_cov(noise, jacobian):\n",
    "    return np.linalg.inv(jacobian) @ noise @ np.linalg.inv(jacobian).T\n",
    "\n",
    "\n",
    "def angle_diff(angle1, angle2):\n",
    "    # Compute the difference between two angles\n",
    "    # using arctan2 to correctly cope with the signs of the angles\n",
    "    return np.arctan2(np.sin(angle1 - angle2), np.cos(angle1 - angle2))\n",
    "\n",
    "\"\"\"\n",
    "1.b\n",
    "\"\"\"\n",
    "def sensor_update(sensor_data, particles):\n",
    "    # Correct landmark poses with a range and bearing measurement and\n",
    "    # compute new particle weight\n",
    "\n",
    "    # Noise of sensor measurements\n",
    "    Q_t = np.array([[0.1, 0],\n",
    "                    [0, 0.1]])\n",
    "\n",
    "    # measured landmark ids and ranges\n",
    "    ids = sensor_data['id']\n",
    "    ranges = sensor_data['range']\n",
    "    bearings = sensor_data['bearing']\n",
    "\n",
    "    # Perform sensor update for each particle\n",
    "    \"\"\"\n",
    "    Line 2\n",
    "    Loop over all particles\n",
    "    \"\"\"\n",
    "    for particle in particles:\n",
    "\n",
    "        \"\"\"\n",
    "        Line 5\n",
    "        landmarks as the observed features\n",
    "        \"\"\"\n",
    "        landmarks = particle['landmarks']\n",
    "\n",
    "        \"\"\"\n",
    "        Line 4\n",
    "        particle pose parameters\n",
    "        \"\"\"\n",
    "        px = particle['x']\n",
    "        py = particle['y']\n",
    "        ptheta = particle['theta']\n",
    "\n",
    "        \"\"\"\n",
    "        Line 5\n",
    "        landmarks as the observed features\n",
    "        \"\"\"\n",
    "        # loop over observed landmarks\n",
    "        for i in range(len(ids)):\n",
    "\n",
    "            \"\"\"\n",
    "            Line 4\n",
    "            landmark parameters\n",
    "            \"\"\"\n",
    "            # current landmark\n",
    "            lm_id = ids[i]\n",
    "            # EKF for current landmark\n",
    "            landmark = landmarks[lm_id]\n",
    "\n",
    "            # measured range and bearing to current landmark\n",
    "            measured_range = ranges[i]\n",
    "            measured_bearing = bearings[i]\n",
    "\n",
    "            \"\"\"\n",
    "            Line 4\n",
    "            calculate landmark position from particle pose and the measurement\n",
    "            \"\"\"\n",
    "            lm_x = px + measured_range * np.cos(ptheta + measured_bearing)\n",
    "            lm_y = py + measured_range * np.sin(ptheta + measured_bearing)\n",
    "\n",
    "            \"\"\"\n",
    "            Line 6\n",
    "            Never seen landmark\n",
    "            \"\"\"\n",
    "            if not landmark['observed']:\n",
    "\n",
    "                \"\"\"\n",
    "                Line 7\n",
    "                initialize the landmark position based on the measurement and particle pose\n",
    "                \"\"\"\n",
    "                landmark['mu'] = [lm_x, lm_y]\n",
    "\n",
    "                \"\"\"\n",
    "                Line 8\n",
    "                calculate jacobian, measurement function h not needed\n",
    "                \"\"\"\n",
    "                _, H = measurement_prediction_and_jacobian(particle=particle,\n",
    "                                                           landmark=landmark)\n",
    "\n",
    "                \"\"\"\n",
    "                Line 9\n",
    "                Initialize landmark covariance (uncertainty)\n",
    "                \"\"\"\n",
    "                landmark['sigma'] = calculate_landmark_cov(noise=Q_t,\n",
    "                                                         jacobian=H)\n",
    "\n",
    "                landmark['observed'] = True\n",
    "\n",
    "            #  landmark was observed before\n",
    "            else:\n",
    "                \"\"\"\n",
    "                EKF Update\n",
    "                \"\"\"\n",
    "                \"\"\"\n",
    "                Line 12 - 13\n",
    "                measure prediction, calculate jacobian\n",
    "                \"\"\"\n",
    "                h, H = measurement_prediction_and_jacobian(particle=particle,\n",
    "                                                           landmark=landmark)\n",
    "\n",
    "                \"\"\"\n",
    "                Line 14\n",
    "                measurement covariance (uncertainty)\n",
    "                \"\"\"\n",
    "                Q = H @ landmark['sigma'] @ H.T + Q_t\n",
    "\n",
    "                \"\"\"\n",
    "                Line 15\n",
    "                calculate kalman gain\n",
    "                \"\"\"\n",
    "                K = landmark['sigma'] @ H.T @ np.linalg.inv(Q)\n",
    "\n",
    "                \"\"\"\n",
    "                Line 16\n",
    "                update mean\n",
    "                \"\"\"\n",
    "                predicted_range = h[0]  # readability\n",
    "                predicted_bearing = h[1]\n",
    "                landmark['mu'] = landmark['mu'] + K @ np.array([(measured_range - predicted_range),\n",
    "                                                                    angle_diff(measured_bearing, predicted_bearing)])\n",
    "\n",
    "                \"\"\"\n",
    "                Line 17\n",
    "                update covariance\n",
    "                \"\"\"\n",
    "                landmark['sigma'] = (np.eye(2) - K @ H) @ landmark['sigma']\n",
    "\n",
    "                \"\"\"\n",
    "                Line 18\n",
    "                update particle weight / importance factor based on likelihood\n",
    "                \"\"\"\n",
    "                likelihood =  scipy.stats.multivariate_normal.pdf(x=[lm_x, lm_y],\n",
    "                                                                          mean=landmark['mu'],\n",
    "                                                                          cov=Q)\n",
    "                particle['weight'] *= likelihood\n",
    "\n",
    "\n",
    "    # normalize weights\n",
    "    normalizer = sum([p['weight'] for p in particles])\n",
    "\n",
    "    for particle in particles:\n",
    "        particle['weight'] = particle['weight'] / normalizer\n",
    "\n",
    "\n",
    "def resampling(particles):\n",
    "    # Returns a new set of particles obtained by stochastic\n",
    "    # universal sampling, according to particle weights.\n",
    "    # (see \"Take Home Exam / Assignment\")\n",
    "\n",
    "    # compute distance between pointers\n",
    "    step = 1.0 / len(particles)\n",
    "\n",
    "    # random position of first pointer\n",
    "    u = np.random.uniform(0, step)\n",
    "\n",
    "    # where we are located along the weights\n",
    "    c = particles[0]['weight']\n",
    "\n",
    "    # index of weight container and corresponding particle\n",
    "    i = 0\n",
    "\n",
    "    new_particles = []\n",
    "\n",
    "    # loop over all particle weights\n",
    "    for particle in particles:\n",
    "\n",
    "        # go through the weights until you find the particle to which the pointer points\n",
    "        while u > c:\n",
    "            i = i + 1\n",
    "            c = c + particles[i]['weight']\n",
    "\n",
    "        # add that particle\n",
    "        new_particle = copy.deepcopy(particles[i])\n",
    "        new_particle['weight'] = 1.0 / len(particles)\n",
    "        new_particles.append(new_particle)\n",
    "\n",
    "        # increment the threshold\n",
    "        u = u + step\n",
    "\n",
    "    return new_particles\n",
    "\n",
    "\n",
    "def main():\n",
    "    # implementation of feature-based FastSLAM\n",
    "\n",
    "    print(\"Reading landmark positions\")\n",
    "    landmarks = read_world_data(\"data/world_data.dat\")\n",
    "\n",
    "    print(\"Reading sensor data\")\n",
    "    sensor_readings = read_sensor_data(\"data/sensor_data.dat\")\n",
    "\n",
    "    num_particles = 100\n",
    "    num_landmarks = len(landmarks)\n",
    "\n",
    "    # create particle set\n",
    "    particles = initialize_particles(num_particles, num_landmarks)\n",
    "\n",
    "    # run FastSLAM for all timestamps\n",
    "    for timestamp in range(len(sensor_readings) // 2):\n",
    "        # predict particles by sampling from motion model with odometry readings\n",
    "        sample_odometry_motion_model(sensor_readings[timestamp, 'odometry'], particles)\n",
    "\n",
    "        # update landmarks and compute particle weights using the measurement model\n",
    "        sensor_update(sensor_readings[timestamp, 'sensor'], particles)\n",
    "\n",
    "        # plot the current FastSLAM state\n",
    "        plot_slam_state(particles, landmarks, timestamp)\n",
    "\n",
    "        # compute new set of particles with equal weights\n",
    "        particles = resampling(particles)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}